# VLA 视觉数据：别被“无损”绑架了，规模化才是王道

> **写在前面**：我们搞具身智能（Embodied AI）的，特别是做 VLA (Vision-Language-Action) 的，最头疼的往往不是模型结构，而是数据。数据少了模型不聪明，数据多了硬盘受不了，训练 IO 还读不动。
>
> 压缩是必须的，但这不仅仅是为了省硬盘。我们的目标不是追求像素级的完美，而是为了让数据规模和迭代速度能上一个量级。本文提到的 foxglove 与 mcap 虽然是基于我们的中间件，但逻辑对 ROS2/rosbag 用户完全通用。
>
> （注：深度图和点云的压缩比较敏感，咱们以后单开一篇聊，这次只谈 RGB。）

---

##  先说结论 

如果你不想听我啰嗦，直接照这个配置来，准没错：

-   **RGB 录制**：直接上 `JPEG`，质量设为 `quality=95`。配合 `foxglove.CompressedImage` 和 MCAP 的 `zstd` chunk 压缩。
-   **Depth 录制**：这个别乱压，用 `foxglove.RawImage` 存 `Z16` 格式，靠 MCAP 的 `zstd` 来无损压缩。
-   **核心判断**：在 VLA / 机器人端到端策略训练里，**JPEG(Q95) 的那点信息损失根本不是瓶颈**；真正制约你模型性能的，是数据的**规模**和**多样性**。
-   **避坑指南**：除非你只是为了录下来给人看回放，否则**千万别**直接存成 H.264/H.265 视频流拿去训练，随机读取的解码成本会教你做人（详见第 7 节）。

---

## 1. 为什么我们要折腾这个？

简单说，就是**痛**。

-   **规模化的痛**：只要你开始搞长时录制，Raw RGB 数据瞬间就能把硬盘填满。
-   **训练的痛**：训练的时候我们需要随机采样（Shuffle），而不是像看电影一样顺序播放。如果数据格式不对，GPU 再快也得等着硬盘和 CPU 解码，显卡利用率 0% 是常有的事。
-   **工程落地的痛**：录制端要求稳，掉帧就是事故；训练端要求快。这就需要我们在“录得稳”和“练得快”之间找个平衡点（比如引入 Zarr，见第 5 节）。

---

## 2. 这里的“水”很深 (背景与挑战)

### 2.1 存储爆炸是会呼吸的痛

当我们从“实验室跑个 Demo”转向“大规模泛化训练”时，数据量级是指数级暴涨的。

-   **算笔账**：双臂机器人，两个 640x480 @ 30Hz 的 RGB 相机，如果是 Raw 数据，**每小时产生约 100 GB** 数据。
-   **后果**：你要是搞个数千小时的数据集，不仅存储成本上天，光是把数据从工控机拷出来、上传云端、分发给队友，都能让你等到地老天荒。
-   **甚至影响安全**：极高的写入带宽需求，甚至会抢占工控机（NUC/Orin）的 IO 资源，导致实时控制循环卡顿——这在机器人上可是要命的。

### 2.2 “画质强迫症”是一种病

我们团队内部一开始也有很强的“保真度焦虑”（Fidelity Anxiety）：

> *“哎呀，JPEG 是有损压缩啊！万一丢失了什么关键纹理怎么办？模型天花板会不会因此下降？”*

这种担心在搞医学影像或者超分重建时是对的，但在 VLA 领域，这纯属**多虑**。现代端到端模型（End-to-End Visuomotor Policies）之所以强，是因为它见多识广（**Diversity**），而不是因为它能看清每一个像素噪点（**Pixel Perfection**）。

---

## 3. 别怕，压缩真的没关系

为什么我说“压缩不影响 VLA”？这不是拍脑袋，是因为训练管线本身就在疯狂“破坏”画面：

### 3.1 Resize 的那一刀，比 JPEG 狠多了
-   **原始采集**：你可能采的是 640×480 甚至更高。
-   **模型输入**：主流模型也就吃 224×224 或者 336×336。
-   **真相**：下采样（Downsampling）丢失的高频信息，比 JPEG(Q95) 丢的那点多得多。像素级完美？不存在的，进模型前早就不完美了。

### 3.2 数据增强：主动“搞破坏”
为了防止过拟合，我们训练时会特意加 ColorJitter（颜色抖动）、Blur（模糊）、RandomResizedCrop（随机裁剪）。模型被迫去学习物体的语义和结构，而不是死记硬背像素值。相比于这些“大动作”，JPEG 带来的轻微伪影简直可以忽略不计。

### 3.3 数据说话
-   单帧质量到底怎么样？详见 [`JPEG保真度分析.md`](https://github.com/Zwhy2025/VLA-Vision-Data-Analysis/blob/master/JPEG保真度分析.md)
-   系统性能提升有多少？详见 [`图像压缩性能分析.md`](https://github.com/Zwhy2025/VLA-Vision-Data-Analysis/blob/master/图像压缩性能分析.md)

---

## 4. 压缩策略：JPEG(Q95) 就是性价比之王

经过各种尝试，我们强烈推荐 **JPEG (Q95) + Zstd** 的组合。

-   **省空间**：对规模化采集至关重要，硬盘得到了解放。
-   **省 CPU**：Q95 编码开销很低；相比之下，PNG 这种无损压缩经常把单核跑满，导致录制频率掉得稀里哗啦。
-   **随机读取友好**：这点最关键，每一帧都是独立的，训练时指哪打哪。

### 4.1 各种方案大乱斗

| 方案 | 特性 | 评价 |
| :--- | :--- | :--- |
| **Raw + Zstd** | 无损，文件级压缩 | **太大了**。除非你是存深度图或者极其宝贵的 Golden Set，否则别用。 |
| **JPEG (Q95) + Zstd** | 有损，帧内压缩 | **永远的神**。体积小 (~1/10 Raw)，画质极好，读取快。通用 RGB 录制首选。 |
| **H.264/H.265 视频** | 有损，帧间压缩 | **看起来很美，用起来很坑**。压缩率确实高，但随机访问极慢（要解码整个 GOP），Dataloader 会到你崩溃。：而且训练速度奇慢，本质上模型训练权衡（Trade-off）磁盘/CPU/显存/内存/IO等资源，不过有兴趣在咱们以后单开一篇聊 |

### 4.2 训练和推理不一致怎么办？
有人会问：*“训练用 JPEG，推理时机器人摄像头直出 Raw，这不就是分布偏移（Distribution Shift）吗？”*

是有那么一点点，但**完全不用慌**：
1.  模型看的是结构和语义，JPEG 改不了这个。
2.  训练时的增强手段早就覆盖了这种微小的画质差异。
3.  **大招**：如果你实在有洁癖，可以在训练的数据增强里，显式地加一个 JPEG 压缩模拟：

```python
# 既然打不过，就加入它
import albumentations as A

transform = A.Compose([
    # 模拟 JPEG 压缩带来的伪影，让模型适应它
    A.ImageCompression(quality_lower=85, quality_upper=100, p=0.5),
    # ... 其他增强
])
```

---

## 5. 进阶玩法：Zarr (利好训练)

录制的时候，为了安全（断电保护）和兼容性，我们通常用 mcap 或 rosbag 这种流式写入格式。但这玩意儿是顺序写的，训练时想随机跳着读，效率极低。

这就到了 **Zarr** 登场的时候了。把它想象成一个巨大的、分块（Chunked）的 Numpy 数组，存放在磁盘上。

### 5.1 为什么 Zarr 是训练神器
-   **指哪读哪**：基于 Chunk 读取，只解码你需要的那一小块数据。
-   **多核狂喜**：天然支持并发读取，多个 Dataloader worker 可以同时读不同的 chunk，互不打架。
-   **压缩灵活**：可以对每个 chunk 单独压缩（zstd, blosc 等），在速度和体积间自由调优。

### 5.2 推荐工作流 (录制存储与训练解耦)
不要试图用一种格式通吃。

1.  **录制/归档**：老老实实由 `mcap` (或 rosbag) 负责，它是“事实来源”（Source of Truth），保证数据不丢、时间戳对齐。
2.  **预处理**：离线把 mcap 解包、Resize、对齐，然后转存为 **Zarr**。
3.  **训练**：直接加载 Zarr。虽然多了一步预处理，但你会在训练时的 Epoch 迭代速度上找补回来，绝对值。

---

## 6. 关于视频流的一点碎碎念

最后聊聊视频。很多同学直觉认为：“机器人动作是时序的，所以我应该把数据存成 mp4 视频，对吧？”

**大错特错。**

### 6.1 “需要时序” ≠ “存成视频文件”
你需要的是“过去几帧的信息”，而不是“H.264 编码格式”。
你完全可以存一串 JPEG 帧，训练时按顺序读出来几张，这就是时序。

### 6.2 视频编码是 Dataloader 的噩梦
H.264/H.265 为了高压缩率，使用了 P 帧和 B 帧（依赖前后帧）。这意味着你想读第 100 帧，解码器可能得先把第 1 到 99 帧都解一遍才能算出来第 100 帧长啥样。
当你在训练中随机采样（Shuffle）时，这种开销是灾难性的。你的 CPU 会在解码中累死，而 GPU 在旁边闲得发慌。

### 6.3 建议
-   **训练用的数据**：保持“逐帧独立”（JPEG 序列 或 Zarr）。
-   **给人看的数据**：如果需要做汇报展示，离线转个 mp4 出来就行，千万别反客为主，让展示格式绑架了训练格式。如果要调试那么rosbag和mcap丰富生态等你探索。
